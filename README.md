# Web Analyzer ğŸŒ

_This project is based on a Web Crawler (in scripts folder), a document scraper and a metadata analyzer (both on scripts folder too). The purpose of the project is to analyze all URLS related to the main one given by the user and analyze its documents._

## Installing ğŸ”§

First clone the repo: 
```
https://github.com/Carliquiss/Document-scaping-Tool.git
```
Then run the following command to install needed libs:
```
pip3 install -r requirements.txt
```

## Usage ğŸš€

To run the tool agains an URL: 
```
python3 WebAnalyzer.py -u url -cq
```
The -u <url> param is needed and it specifies the URL to scan. The other params -c (clean older folders generated by the script) and -q (run in quiet mode) are optional. 

When an URL is scanned you will see 4 folders: Documents, Metadata, URLs_externas and URLs_internas. 
 * **Documents**: Here are all downloaded documents found in the URL
 * **Metadata**: Where you will find the results of the documents metadata analysis (in he inform_xxxx file you will find all the documents with sensible metadata, and in the Data_xxxx file all the metadata from the documents)
 * **URLS_externals**: External URLS found spiding the web URL given
 * **URLS_locals**: Locals URLs (from the URL given by the user) found spiding
